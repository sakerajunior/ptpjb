{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"green\"> https://bit.ly/ptpjb-2021-14</font></center>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://bit.ly/ptpjb-2021-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">14 - Deep Learning ~ LSTM</font>\n",
    "\n",
    "<center><img alt=\"\" src=\"images/cover_ptpjb_2021.png\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>Fathu Rahman - 2021</center>\n",
    "<center><a href=\"https://tau-data.id\">https://tau-data.id</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "<br>\n",
    "<table><tr>\n",
    "    <td> <img src=\"images/handwriting.png\" width=\"300\" align = left/> </td>\n",
    "    <td> <img src=\"images/speech recognition.jpg\" width=\"300\" align = left/> </td>\n",
    "    <td> <img src=\"images/anomaly detection.png\" width=\"300\" align = left/> </td>\n",
    "</tr></table>\n",
    "<br>\n",
    "Long short-term memory (LSTM) merupakan arsitektur Recurrent Neural Network (RNN). Tidak seperti feed forward neural network biasa, LSTM memiliki koneksi umpan balik sehingga LSTM dapat memproses tidak hanya data tunggal (seperti gambar), tetapi juga seluruh urutan data (seperti ucapan atau video). LSTM mampu melakukan tugas-tugas seperti handwriting recognition, speech recognition dan anomaly detection pada network traffic atau IDSs (intrusion detection systems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feed-forward Neural Network Vs Recurrent Neural Network\n",
    "<img src=\"images/Comparison-of-FFNN-and-RNN.png\" />\n",
    "\n",
    "image source: https://www.researchgate.net/figure/Comparison-of-FFNN-and-RNN_fig1_320944634\n",
    "\n",
    "- **Arsitektur FFNN hanya menjadikan input sebagai kombinasi linear dan dimasukan ke fungsi aktivasi**\n",
    "- **Input pada arsitektur RNN adalah data sekuensial sehingga tidak tepat jika dijadikan sebagai kombinasi linear**\n",
    "- **Saat unit RNN dibentangkan, kita akan melihat node-node RNN yang disebut dengan blok RNN**\n",
    "- **Blok RNN ini berfungsi untuk mengalirkan informasi dari data urutan pertama hingga terakhir, sehingga output tiap-tiap bloknya akan bergantung pada data di blok tersebut dan seluruh data pada blok sebelumnya**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perhatikan Data Berikut\n",
    "\n",
    "<img src=\"images/f-img/data_rnn.JPG\"/>\n",
    "\n",
    "## - FFNN\n",
    "<img src=\"images/f-img/data_ffnn.JPG\" width=\"800\"/>\n",
    "\n",
    "## - RNN\n",
    "<img src=\"images/f-img/data_rnn1.JPG\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blok RNN\n",
    "\n",
    "- Di dalam blok RNN terdapat fungsi aktivasi tanh yang memaksa nilai outputnya berada di interval -1 dan 1\n",
    "\n",
    "<img src=\"images/f-img/RNN.JPG\" width=\"800\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arsitektur RNN\n",
    "\n",
    "<img src=\"images/f-img/rnn_vs_ffn.png\" width=\"600\" />\n",
    "\n",
    "### - 1 hidden layer dengan 1 unit RNN\n",
    "\n",
    "<img src=\"images/f-img/rnn_layer1.JPG\" width=\"800\" />\n",
    "\n",
    "### - 1 hidden layer dengan 2 unit RNN\n",
    "\n",
    "<img src=\"images/f-img/rnn_layer2.JPG\" width=\"800\" />\n",
    "\n",
    "### - 2 hidden layer dengan 1 unit RNN\n",
    "\n",
    "<img src=\"images/f-img/rnn_layer3.JPG\" width=\"600\" />\n",
    "\n",
    "### - Hidden layer 1 dengan 2 unit RNN dan hidden layer 2 dengan 1 unit RNN\n",
    "\n",
    "<img src=\"images/f-img/rnn_layer4.JPG\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Vs RNN\n",
    "<img src=\"images/ilustrasi_review_lstm.jpg\"/>\n",
    "\n",
    "<img src=\"images/f-img/rnn vs lstm.JPG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cara kerja LSTM\n",
    "<img src=\"images/forget gate.gif\" width=\"700\" />\n",
    "<img src=\"images/input gate.gif\" width=700 />\n",
    "<img src=\"images/cell state.gif\" width=700 />\n",
    "<img src=\"images/output gate.gif\" width=700 />\n",
    "\n",
    "image source:https://www.megabagus.id/deep-learning-recurrent-neural-networks/4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for forecasting\n",
    "\n",
    "- Misalkan kita memiliki data time series sebagai berikut:\n",
    "<img src=\"images/f-img/time_series_data.JPG\" width=250 />\n",
    "\n",
    "- Untuk melakukan forecasting menggunakan LSTM pada data tersebut, hal yang perlu dilakukan adalah menjadikan data tersebut menjadi data sekuensial menggunakan sliding window.\n",
    "\n",
    "- Sliding Window: Misalkan window size = w, data ke $t$ diprediksi dengan melihat w data sebelumnya.\n",
    "\n",
    "- Sebagai ilustrasi, perhatikan animasi berikut:\n",
    "<img src=\"images/sliding window.gif\" width=700 />\n",
    "\n",
    "- Apabila kita memilih window size = 5, setelah dilakukan sliding window pada data time series yang diperlihatkan di awal, maka akan kita dapatkan data sebagai berikut:\n",
    "<img src=\"images/f-img/sekuensial_data.JPG\" width=250 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studi Kasus\n",
    "Sebagai contoh kita akan membuat model forecasting sederhana dengan menggunakan dataset \n",
    "\n",
    "**\"Hourly Energy Consumption: Over 10 years of hourly energy consumption data from PJM in Megawatts\"**\n",
    "\n",
    "PJM Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system serving all or parts of Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, North Carolina, Ohio, Pennsylvania, Tennessee, Virginia, West Virginia, and the District of Columbia.\n",
    "\n",
    "The hourly power consumption data comes from PJM's website and are in megawatts (MW).\n",
    "\n",
    "The regions have changed over the years so data may only appear for certain dates per region.\n",
    "\n",
    "Source: https://www.kaggle.com/robikscube/hourly-energy-consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat 14 data pada dataset tersebut. yang digunakan pada contoh ini adalah data AEP_hourly.csv yaitu data konsumsi energi dari American Electric Power (AEP) dalam satuan megawatt (MW)\n",
    "\n",
    "<img src=\"images/AEP_logo.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modul Standar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mempersiapkan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data Konsumsi Energi dari American Electric Power (AEP)\n",
    "\n",
    "- Data konsumsi energi dari American Electric Power (AEP) diimport menggunakan pandas\n",
    "- Karena format datanya csv maka untuk mengimportnya menggunakan `pd.read_csv()`\n",
    "- Setelah diimport dan simpan dalam variabel df, kita coba lihat 10 data teratas menggunakan `df.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = 'data/AEP_hourly.csv'\n",
    "try: # Running Locally\n",
    "    df = pd.read_csv(file_)\n",
    "except: # Running in Google Colab\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/main/{file_}\n",
    "    df = pd.read_csv(file_)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melihat informasi singkat dari dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dari informasi singkat di atas, kita dapat mengetahui bahwa df terdiri dari 121273 baris. \n",
    "- Selain itu, pada kolom Datetime, tipe datanya masih berupa object (string) sehingga perlu diubah menjadi tipe data datetime dengan cara berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merubah tipe data object to datetime\n",
    "df['Datetime'] = df['Datetime'].astype('datetime64')\n",
    "\n",
    "# melihat tipe data dataframe\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhatikan waktu terawal dari kolom Datetime dan 5 data teratas dari df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('waktu terawal dari kolom Datetime adalah:', df['Datetime'].min())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dapat kita lihat bahwa waktu pada baris pertama dari kolom tidak sama dengan waktu terawalnya. \n",
    "- Maka dari itu perlu kita urutkan df berdasarkan Datetime menggunakan `df.sort_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengurutkan data berdasarkan waktu\n",
    "df.sort_values('Datetime', inplace=True, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=df, x='Datetime', y='AEP_MW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memilih Data Setahun Terakhir\n",
    "- Pada contoh ini kita hanya akan gunakan data setahun terakhir dari data AEP_hourly\n",
    "- Karena data yang diobservasi per jam, maka kita akan mengambil 24*365=8760 baris terakhir dari df dan dimasukan kedalam variabel df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[-24*365:].reset_index(drop=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=df1, x='Datetime', y='AEP_MW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melihat Statistika Deskriptif dari Data\n",
    "- Sebelum melakukan pembuatan model, sebaiknya dilakukan analisa terhadap statistika deskriptif dari data\n",
    "- Dari statistika deskriptif tersebut, kita dapat meilhat range dari data dan ukuran pusat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dari statistika deskriptif di atas terlihat bahwa data AEP_MW cukup besar dan berada pada range 9801 dan 22759 sehingga nanti kita akan lakukan **feature scalling** menggunakan **MinMaxScaler** agar range dari seluruh data tersebut berada di antara 0 dan 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "<img src=\"images/cross_validation.png\">\n",
    "\n",
    "- Split data dilakukan agar model yang telah dilatih dapat dievaluasi kemampuannya.\n",
    "- Karena data yang digunakan adalah data time series, maka split data tidak dilakukan secara acak\n",
    "- Kita juga akan melakukan **cross validation** menggunakan data train sehingga pastikan data train yang digunakan cukup besar.\n",
    "- Pada contoh ini kita gunakan 70% baris pertama sebagai data train dan 30% sisanya sebagai data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_size = int(len(df1) * 0.7) # Menentukan banyaknya data train yaitu sebesar 70% data\n",
    "train = df1[:train_size]\n",
    "test =df1[train_size:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling Menggunakan MinMaxScaler\n",
    "- MinMaxScaler difit pada data train agar dapat digunakan kembali pada data test maupun data observasi baru.\n",
    "- Hasil scalling disimpan pada kolom baru yaitu `'scaled'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['AEP_MW']])\n",
    "\n",
    "train['scaled'] = scaler.transform(train[['AEP_MW']])\n",
    "test['scaled'] = scaler.transform(test[['AEP_MW']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mari kita lihat 5 data pertama pada data train untuk melihat data yang sudah discalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat fungsi sliding window\n",
    "- Selanjutnya kita akan membuat fungsi sliding window dengan input data (bertipe data numpy array) dan window size\n",
    "- Fungsi ini akan menghasilkan variabel input (X) dan variabel target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data, window_size):\n",
    "    sub_seq, next_values = [], []\n",
    "    for i in range(len(data)-window_size):\n",
    "        sub_seq.append(data[i:i+window_size])\n",
    "        next_values.append(data[i+window_size])\n",
    "    X = np.stack(sub_seq)\n",
    "    y = np.array(next_values)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berapa window size yang tepat untuk digunakan?\n",
    "\n",
    "- Pada penerapannya kita dapat menentukan window size berapa saja. \n",
    "- Untuk mencapai hasil yang maksimal dapat dilakukan percobaan dengan menggunakan beberapa window size. \n",
    "- Perlu diperhatikan juga bahwa semakin besar window size yang digunakan akan memerlukan waktu yang cukup lama dalam memproses data\n",
    "- Pada contoh ini kita hanya menggunakan **window size = 24** atau sama dengan 1 hari dan kita terapkan pada data train dan test yang telah discalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 24\n",
    "\n",
    "X_train, y_train = sliding_window(train[['scaled']].values, window_size)\n",
    "X_test, y_test = sliding_window(test[['scaled']].values, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penting!!! \n",
    "- Data input LSTM harus 3D : [samples, timesteps, feature]\n",
    "- Maka dari itu kita perlu cek dimensi data kita menggunakan np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM menggunakan Tensorflow dan Keras\n",
    "- tensorflow: https://www.tensorflow.org/overview\n",
    "- keras: https://keras.io/about/\n",
    "<br><br>\n",
    "- **LSTM()**, perhatikan parameter-parameter berikut untuk membuat LSTM layer sederhana seperti yang telah dijelaskan sebelumnya menggunakan Keras:\n",
    "1. units: menentukan banyaknya LSTM unit\n",
    "2. input_shape: menentukan ukuran timesteps dan feature, diperlukan pada layer pertama\n",
    "3. return_sequences: jika layer berikutnya berupa LSTM layer maka return_sequences=True (default = False)\n",
    "\n",
    "Paramaeter-parameter lainnya dapat dilihat pada link berikut: https://keras.io/api/layers/recurrent_layers/lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membuat Model Forecasting Menggunakan LSTM\n",
    "### 1. Import Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Membuat Fungsi Model Forecasting Menggunakan LSTM\n",
    "\n",
    "Fungsi model yang akan dibuat terdiri:\n",
    "- LSTM layer dengan input_shape = (window_size, 1)\n",
    "- Dense layer dengan 32 neuron dengan fungsi aktivasi ReLu\n",
    "- Dropout layer antara Dense layer dan Dense output layer\n",
    "- Dense output layer dengan 1 neuron\n",
    "- loss function yang digunakan adalah Mean Squared Error (MSE)\n",
    "- optimizer yang digunakan adalah adam\n",
    "- metric yang digunakan adalah Mean Absolute Error (MAE)\n",
    "\n",
    "Parameter-parameter yang dijadikan sebagai input dari fungsi tersebut adalah:\n",
    "- LSTM_unit: banyaknya LSTM unit (default = 64)\n",
    "- dropout: persentase dropout (default = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(LSTM_unit=64, dropout=0.2):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=LSTM_unit, input_shape=(window_size, 1)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Membuat Model\n",
    "\n",
    "Kita coba lakukan hypertuning pada parameter berikut seperti yang dilakukan pada Modul ke 13:\n",
    "- LSTM_unit = [16,32,64,128]\n",
    "- dropout = [0.1,0.2]\n",
    "\n",
    "Selain itu, kita juga gunakan early stopping pada saat proses training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = \"min\", patience = 5, verbose = 0)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=50, validation_split=0.1, batch_size=32, callbacks=[es], verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "LSTM_unit = [16,32,64,128]\n",
    "dropout=[0.1,0.2]\n",
    "param_grid = dict(LSTM_unit=LSTM_unit, dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Membuat Variabel GridSearchCV\n",
    "Variabel GridSearchCV dibuat dengan memasukan beberapa parameter yaitu:\n",
    "- estimator: model yang ingin dilakukan gridsearch\n",
    "- param_grid: parameter yang ingin diuji\n",
    "- n_jobs: Jumlah pekerjaan untuk dijalankan secara paralel. (-1 artinya menggunakan seluruh core processor)\n",
    "- cv: banyaknya k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training Model dengan GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Melihat Hasil Parameter Terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# Mengambil model terbaik\n",
    "best_model = grid_result.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari Hasil Training menggunakan GridSearchCV, kita peroleh:\n",
    "- parameter terbaiknya adalah: {'LSTM_unit': 32, 'dropout': 0.1}\n",
    "- Rata-rata Loss Function dari hasil Cross Validation adalah 0.000393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kemudian coba kita lihat grafik loss function MSE dan metric MAE terhadap epoch untuk melihat performa model terbaik kita dengan cara sebagai berikut**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat melihat grafik loss function MSE dan metric MAE terhadap epoch untuk melihat performa model kita dengan cara sebagai berikut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafik loss function MSE\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('loss function MSE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafik metric MAE\n",
    "\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('metric MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Melakukan prediksi pada data train dan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best_model\n",
    "import keras\n",
    "\n",
    "folder_ = 'LSTM_forecasting'\n",
    "try: # load Locally\n",
    "    best_model = keras.models.load_model(folder_)\n",
    "except: # load in Google Colab\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/ptpjb/main/{folder_}.zip\n",
    "    !unzip /content/{folder_}.zip\n",
    "    best_model = keras.models.load_model(folder_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi data train\n",
    "predict_train = scaler.inverse_transform(best_model.predict(X_train))\n",
    "true_train = scaler.inverse_transform(y_train)\n",
    "\n",
    "# Prediksi data test\n",
    "predict_test = scaler.inverse_transform(best_model.predict(X_test))\n",
    "true_test = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setelah melakukan prediksi barulah kita melakukan evaluasi terhadap nilai prediksi tersebut menggunakan metric yang digunakan yaitu MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE) data train\n",
    "mae_train = np.mean(np.abs(true_train-predict_train))\n",
    "print('MAE data train sebesar:', mae_train)\n",
    "\n",
    "# Mean Absolute Error (MAE) test data\n",
    "mae_test = np.mean(np.abs(true_test-predict_test))\n",
    "print('MAE data test sebesar:', mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apakah Nilai MAE Tersebut Bagus???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melihat boxplot dari nilai error mutlak**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_error_train = np.abs(true_train-predict_train)\n",
    "sns.boxplot(y=abs_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_error_test = np.abs(true_test-predict_test)\n",
    "sns.boxplot(y=abs_error_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melihat range data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('range data train', true_train.max()-true_train.min())\n",
    "print('range data test', true_test.max()-true_test.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediksi data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predict'] = np.nan\n",
    "train['predict'][-len(predict_train):] = predict_train[:,0]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=train, x='Datetime', y='AEP_MW', label = 'train')\n",
    "sns.lineplot(data=train, x='Datetime', y='predict', label = 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediksi data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predict'] = np.nan\n",
    "test['predict'][-len(predict_test):] = predict_test[:,0]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=test, x='Datetime', y='AEP_MW', label = 'test')\n",
    "sns.lineplot(data=test, x='Datetime', y='predict', label = 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediksi data test sebulan terakhir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=test[-24*30:], x='Datetime', y='AEP_MW', label = 'test')\n",
    "sns.lineplot(data=test[-24*30:], x='Datetime', y='predict', label = 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melakukan forecasting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecasting data selanjutnya\n",
    "y_test = scaler.transform(test[['AEP_MW']])\n",
    "n_future = 24*7\n",
    "future = [[y_test[-1,0]]]\n",
    "X_new = y_test[-window_size:,0].tolist()\n",
    "\n",
    "for i in range(n_future):\n",
    "    y_future = best_model.predict(np.array([X_new]).reshape(1,window_size,1))\n",
    "    future.append([y_future[0,0]])\n",
    "    X_new = X_new[1:]\n",
    "    X_new.append(y_future[0,0])\n",
    "\n",
    "future = scaler.inverse_transform(np.array(future))\n",
    "date_future = pd.date_range(start=test['Datetime'].values[-1], periods=n_future+1, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Data sebulan terakhir dan seminggu ke depan\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=test[-24*30:], x='Datetime', y='AEP_MW', label = 'test')\n",
    "sns.lineplot(data=test[-24*30:], x='Datetime', y='predict', label = 'predict')\n",
    "sns.lineplot(x=date_future, y=future[:,0], label = 'future')\n",
    "plt.ylabel('AEP_MW');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deteksi Anomali\n",
    "\n",
    "<img src=\"images/anomaly_detection.png\"/>\n",
    "\n",
    "- Model forecasting yang telah dibuat, dapat dikembangkan lagi untuk melakukan **deteksti anomali** dengan cara Unsupervised ataupun Supervised. \n",
    "- Namun untuk deteksi anomali secara supervised, anomali pada data time series yang diobservasi perlu ditandai dulu\n",
    "\n",
    "Berikut adalah konsep dasar deteksi anomali secara unsupervised dan supervised:\n",
    "- Unsupervised: \n",
    "1. Model forecasting dibuat menggunakan data train.\n",
    "2. Lakukan prediksi pada data train untuk menghitung error mutlak.\n",
    "3. Tentukan threshold (nilai batas) berdasarkan distribusi error mutlak dari data train.  \n",
    "4. Kemudian lakukan prediksi pada data test.\n",
    "5. Data test yang memiliki nilai error mutlak melebihi threshold tersebut akan diindikasikan sebagai anomali.\n",
    "<br>\n",
    "<br>\n",
    "- Supervised: \n",
    "1. Model forecasting dibuat menggunakan data train.\n",
    "2. Lakukan prediksi pada data test untuk menghitung error mutlak.\n",
    "3. Misal terdapat *n* anomali pada data test, maka threshold dipilih dengan cara memilih nilai mutlak terbesar ke *n* pada data test.\n",
    "4. Data test yang memiliki nilai error mutlak melebihi threshold tersebut akan diindikasikan sebagai anomali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deteksi Anomali (Unsupervised)\n",
    "\n",
    "- Kita akan melakukan deteksi anomali secara unsupervised dengan menggunakan model forecasting yang telah kita buat sebelumnya\n",
    "- Karena kita telah menghitung nilai error mutlak pada data train dan data test, maka selanjutnya kita akan menentukan threshold\n",
    "- Mari kita lakukan plot histogram pada error mutlak data train tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs_error_train, bins=50)\n",
    "plt.xlabel('Error Mutlak')\n",
    "plt.ylabel('Banyaknya Sampel')\n",
    "plt.title('Error Mutlak Data Train');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dari histogram tersebut dapat ditetapkan threshold = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 750\n",
    "print(f'threshold error mutlak: {threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mari kita lihat  juga plot histogram error mutlak pada data test untuk melihat ketepatan pemilihan threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs_error_test, bins=50)\n",
    "plt.xlabel('Error Mutlak')\n",
    "plt.ylabel('Banyaknya Sampel')\n",
    "plt.title('Error Mutlak Data Test');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terlihat bahwa threshold yang dipilih sudah cukup tepat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selanjutnya data test yang memiliki nilai error mutlak melebihi threshold tersebut akan diindikasikan sebagai anomali.\n",
    "- Dan kita masukan ke dalam dataframe test_anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly = pd.DataFrame()\n",
    "test_anomaly['AEP_MW'] = test['AEP_MW'][window_size:]\n",
    "test_anomaly['Datetime'] = test['Datetime'][window_size:]\n",
    "test_anomaly['abs_error'] = abs_error_test\n",
    "test_anomaly['anomaly_hat'] = 0\n",
    "test_anomaly.loc[test_anomaly.abs_error >= threshold, 'anomaly_hat'] = 1\n",
    "test_anomaly.index = test[window_size:].index\n",
    "\n",
    "test_anomaly.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_pred = test_anomaly.loc[test_anomaly['anomaly_hat'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafik data test beserta anomali\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_anomaly.index, y=test_anomaly['AEP_MW'], name='AEP_MW'))\n",
    "fig.add_trace(go.Scatter(x=anomalies_pred.index, y=anomalies_pred['AEP_MW'], mode='markers', name='Anomaly Pred'))\n",
    "fig.update_layout(showlegend=True, title='Detected anomalies')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deteksi Anomali (Supervised)\n",
    "\n",
    "Untuk contoh deteksi anomali dengan cara supervised, kita akan gunakan data web traffic dari **Yahoo Webscope**\n",
    "\n",
    "<img src=\"images/webscope.jpg\" />\n",
    "\n",
    "- The Yahoo Webscope Program is a reference library of interesting and scientifically useful datasets for non-commercial use by academics and other scientists.  \n",
    "- All datasets have been reviewed to conform to Yahoo's data protection standards, including strict controls on privacy.\n",
    "- We have a number of datasets that we are excited to share with you.  \n",
    "- Yahoo is pleased to make these datasets available to researchers who are advancing the state of knowledge and understanding in web sciences. \n",
    "- The datasets are only available for academic use by faculty and university researchers who agree to the Data Sharing Agreement.\n",
    "- More information about the Yahoo! Webscope program is\n",
    "available at http://research.yahoo.com \n",
    "\n",
    "**Dataset: ydata-labeled-time-series-anomalies-v1_0**\n",
    "\n",
    "- Pada dataset tersebut terdiri dari 4 folder dengan total 371 file berisikan data web traffic:\n",
    "- Folder A1Benchmark berisikan 67 data rill web traffic dari beberapa properti Yahoo!.\n",
    "- Kita akan memilih salah satu data yang memiliki anomali terbanyak dari ke 67 data tersebut.\n",
    "\n",
    "**<center>Salah satu data dari folder A1Benchmark (real_1.csv)</center>**\n",
    "<img src=\"images/f-img/data_webtraffic.JPG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplot banyaknya anomali dari ke 67 data pada folder A1Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_anom = []\n",
    "for i in range(67):\n",
    "    file_ = \"data/web_traffic_anomaly/A1Benchmark/real_\"+str(i+1)+\".csv\"\n",
    "    try: # Running Locally\n",
    "        df = pd.read_csv(file_)\n",
    "    except: # Running in Google Colab\n",
    "        !mkdir data\n",
    "        !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/main/{file_}\n",
    "        df = pd.read_csv(\"data/real_\"+str(i+1)+\".csv\")\n",
    "    \n",
    "    N_anom.append(sum(df['is_anomaly']))\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(x=[i for i in range(1,68)], y=N_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Terlihat bahwa data yang memiliki anomali terbanyak adalah data ke 17 dan ke 19.\n",
    "- Pada contoh ini kita coba gunakan data ke 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = \"data/web_traffic_anomaly/A1Benchmark/real_17.csv\"\n",
    "try: # Running Locally\n",
    "    df = pd.read_csv(file_)\n",
    "except: # Running in Google Colab\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/main/{file_}\n",
    "    df = pd.read_csv(\"data/real_17.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kemudian coba kita plot datanya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Data\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=df,x='timestamp', y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Terlihat jelas bahwa range data cukup besar sehingga nanti kita akan lakukan **feature scalling** menggunakan **MinMaxScaler** agar range dari seluruh data tersebut berada di antara 0 dan 1\n",
    "- Jika kita perhatikan seharusnya anomali terjadi pada interval timestamp sekitar [775,875], [1000,1050], dan [1350:]\n",
    "- Mari kita plot juga anomali pada data tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.scatterplot(data=df.loc[df['is_anomaly']==1],x='timestamp', y='value', color='red')\n",
    "sns.lineplot(data=df,x='timestamp', y='value')\n",
    "print('Anomali pertama terjadi pada timestamp ke', df.loc[df['is_anomaly']==1, 'timestamp'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "- Karena anomali pertama terjadi pada timestamp ke 769, maka kita akan gunakan data dengan timestamp < 769-30 (karena kita ingin menggunakan window size = 30) sebagai data train, dan sisanya sebagai data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train = df[:738]\n",
    "test = df[738:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scalling Menggunakan MinMaxScaler\n",
    "- MinMaxScaler difit pada data train agar dapat digunakan kembali pada data test maupun data observasi baru.\n",
    "- Hasil scalling disimpan pada kolom baru yaitu `'scaled'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['value']])\n",
    "\n",
    "train['scaled'] = scaler.transform(train[['value']])\n",
    "test['scaled'] = scaler.transform(test[['value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat variabel input dan ouput menggunakan sliding window\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "X_train, y_train = sliding_window(train[['scaled']].values, window_size)\n",
    "X_test, y_test = sliding_window(test[['scaled']].values, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat model forecasting seperti sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = \"min\", patience = 5, verbose = 0)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=50, validation_split=0.1, batch_size=32, callbacks=[es], verbose=1)\n",
    "\n",
    "# define the grid search parameters\n",
    "LSTM_unit = [64,128]\n",
    "dropout=[0.1,0.2]\n",
    "param_grid = dict(LSTM_unit=LSTM_unit, dropout=dropout)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv = 5)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# Mengambil model terbaik\n",
    "best_model = grid_result.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kemudian coba kita lihat grafik loss function MSE dan metric MAE terhadap epoch untuk melihat performa model terbaik kita dengan cara sebagai berikut**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat melihat grafik loss function MSE dan metric MAE terhadap epoch untuk melihat performa model kita dengan cara sebagai berikut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafik loss function MSE\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('loss function MSE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafik metric MAE\n",
    "\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('metric MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Melakukan prediksi pada data train dan data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best_model\n",
    "import keras\n",
    "\n",
    "folder_ = 'anomaly_detection'\n",
    "try: # load Locally\n",
    "    best_model = keras.models.load_model('streamlit/'+folder_)\n",
    "except: # load in Google Colab\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/ptpjb/main/{folder_}.zip\n",
    "    !unzip /content/{folder_}.zip\n",
    "    best_model = keras.models.load_model(folder_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi data train\n",
    "predict_train = scaler.inverse_transform(best_model.predict(X_train))\n",
    "true_train = scaler.inverse_transform(y_train)\n",
    "\n",
    "# Prediksi data test\n",
    "predict_test = scaler.inverse_transform(best_model.predict(X_test))\n",
    "true_test = scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setelah melakukan prediksi barulah kita melakukan evaluasi terhadap nilai prediksi tersebut menggunakan metric yang digunakan yaitu MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE) data train\n",
    "mae_train = np.mean(np.abs(true_train-predict_train))\n",
    "print('MAE data train sebesar:', mae_train)\n",
    "\n",
    "# Mean Absolute Error (MAE) test data\n",
    "mae_test = np.mean(np.abs(true_test-predict_test))\n",
    "print('MAE data test sebesar:', mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melihat boxplot dari nilai error mutlak**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_error_train = np.abs(true_train-predict_train)\n",
    "sns.boxplot(y=abs_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_error_test = np.abs(true_test-predict_test)\n",
    "sns.boxplot(y=abs_error_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediksi data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predict'] = np.nan\n",
    "train['predict'][-len(y_train):] = predict_train[:,0]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x=train.timestamp, y=train.value, label = 'value')\n",
    "sns.lineplot(x=train.timestamp, y=train.predict, label = 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot prediksi data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predict'] = np.nan\n",
    "test['predict'][-len(y_test):] = predict_test[:,0]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.lineplot(x=test.timestamp, y=test.value, label = 'value')\n",
    "sns.lineplot(x=test.timestamp, y=test.predict, label = 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menentukan anomali pada data test\n",
    "\n",
    "- Misal terdapat n anomali pada data test, maka threshold dipilih dengan cara memilih nilai mutlak terbesar ke n pada data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(abs_error_test, bins=50)\n",
    "plt.xlabel('Error Mutlak')\n",
    "plt.ylabel('Banyaknya Sampel')\n",
    "\n",
    "N_anom_test = sum(test['is_anomaly'][window_size:])\n",
    "print(f'Banyaknya anomali pada data test: {N_anom_test}')\n",
    "threshold = sorted(abs_error_test[:,0])[-N_anom_test]\n",
    "print(f'threshold error mutlak: {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anomaly = pd.DataFrame()\n",
    "test_anomaly['timestamp'] = test['timestamp'][window_size:]\n",
    "test_anomaly['value'] = test['value'][window_size:]\n",
    "test_anomaly['abs_error'] = abs_error_test\n",
    "test_anomaly['is_anomaly'] = test['is_anomaly'][window_size:]\n",
    "test_anomaly['anomaly_hat'] = 0\n",
    "test_anomaly.loc[test_anomaly['abs_error'] >= threshold, 'anomaly_hat'] = 1\n",
    "\n",
    "test_anomaly.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Hasil Deteksi Anomali\n",
    "\n",
    "- Setelah dilakukan deteksi anomali, kita akan evaluasi dengan melihat nilai recall dan presisi.\n",
    "- Nilai yang sangat harus diperhatikan adalah recall karena nilai ini adalah persentase anomali sebenarnya yang terprediksi dengan benar\n",
    "- Perlu diketahui bahwa kita dapat mengatur kembali threshold error mutlak pada data test, asalkan dapat meningkatkan nilai recall tanpa mengurangi nilai presisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "matrix = confusion_matrix(test_anomaly['is_anomaly'], test_anomaly['anomaly_hat'])\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(test_anomaly['is_anomaly'], test_anomaly['anomaly_hat'])\n",
    "\n",
    "# precision: tp / (tp + fp)\n",
    "precision = precision_score(test_anomaly['is_anomaly'], test_anomaly['anomaly_hat'])\n",
    "\n",
    "print(matrix)\n",
    "print('Recall: %f' % recall)\n",
    "print('Precision: %f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_true = test_anomaly.loc[test_anomaly['is_anomaly'] == 1]\n",
    "anomalies_pred = test_anomaly.loc[test_anomaly['anomaly_hat'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# grafik data test beserta anomali\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_anomaly['timestamp'], y=test_anomaly['value'], name='value'))\n",
    "fig.add_trace(go.Scatter(x=anomalies_true['timestamp'], y=anomalies_true['value'], mode='markers', name='Anomaly True'))\n",
    "fig.add_trace(go.Scatter(x=anomalies_pred['timestamp'], y=anomalies_pred['value'], mode='markers', name='Anomaly Pred'))\n",
    "fig.update_layout(showlegend=True, title='Detected anomalies')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latihan\n",
    "\n",
    "## Lakukanlah deteksi anomali secara supervised menggunakan LSTM pada data berikut!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = \"data/web_traffic_anomaly/A1Benchmark/real_19.csv\"\n",
    "try: # Running Locally\n",
    "    df = pd.read_csv(file_)\n",
    "except: # Running in Google Colab\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/main/{file_}\n",
    "    df = pd.read_csv(\"data/real_19.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # untuk deployment anomaly detection\n",
    "\n",
    "# # save model\n",
    "# best_model.save('streamlit/anomaly_detection')\n",
    "\n",
    "# # save scaler\n",
    "# import pickle\n",
    "# pickle.dump(scaler, open('streamlit/scaler.pkl', 'wb'))\n",
    "\n",
    "# # save test data\n",
    "# test[['timestamp','value','is_anomaly']].to_csv('streamlit/real_19_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Akhir Modul 13 Introduction to Network Model and Tensorflow</font></center>\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/meme_07_train_Model.jpg\" style=\"height: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
