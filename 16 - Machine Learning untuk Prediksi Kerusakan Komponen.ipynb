{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"green\"> https://bit.ly/ptpjb-2021-16</font></center>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://bit.ly/ptpjb-2021-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">16 Pendahuluan Machine Learning untuk Prediksi Kerusakan Komponen</font></center>\n",
    "\n",
    "<center><img alt=\"\" src=\"images/cover_ptpjb_2021.png\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>(C) Taufik Edy Sutanto - 2021</center>\n",
    "<center><a href=\"https://tau-data.id\">https://tau-data.id</a> ~ <a href=\"mailto:taufik@tau-data.id\">taufik@tau-data.id</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Outline Module 16</font></center>\n",
    "\n",
    "* Pendahuluan Model Prediksi Kerusakan Komponen (predictive Component)\n",
    "* Pendekatan Stasioner\n",
    "* Pendekatan Time Series\n",
    "* Contoh Penerapan:\n",
    " - Feature Tools\n",
    " - Deep Learning - CNN\n",
    "* Diskusi\n",
    "\n",
    "<p><center><img src=\"images/predictive_maintenance_outline_pic.jpg\" alt=\"\" width=\"500\" height=\"238\" /></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> References </font></center>\n",
    "\n",
    "**Disclaimer**: Beberapa informasi di Module ini adalah kutipan langsung dan tidak langsung dari berbagai sumber berikut (dan sumber yang diberikan pada setiap cell):\n",
    "\n",
    "* https://towardsdatascience.com/how-to-implement-machine-learning-for-predictive-maintenance-4633cdbe4860\n",
    "* https://medium.com/swlh/machine-learning-for-equipment-failure-prediction-and-predictive-maintenance-pm-e72b1ce42da1\n",
    "* https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* https://towardsdatascience.com/remaining-life-estimation-with-keras-2334514f9c61\n",
    "* https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\n",
    "* https://www.kaggle.com/r17sha/mtp-cmapps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Equipment Failure Prediction (EFP) and Preventive Maintenance (PM)</font></center>\n",
    "\n",
    "* **Tujuan**: Data dikumpulkan dari waktu ke waktu untuk mengawasi/monitor keadaan dari suatu komponen/alat. Tujuan dari EFP-PM adalah menemukan pola yang akan dapat membantu prediksi/peramalan terkait pencegahan kerusakan komponen/alat.\n",
    "* **Contoh Aplikasi**:\n",
    "\n",
    "<img src=\"images/contoh_aplikasi_predictive_maintenance.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Preventive Maintenance (PM)</font></center>\n",
    "\n",
    "* Sumber: https://www.getmaintainx.com/blog/what-is-preventative-maintenance/\n",
    "        \n",
    "<img src=\"images/preventive-maintenance-benefits-chart.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Remaining Useful Life - RUL</font></center>\n",
    "\n",
    "*  SOH: State of Health.\n",
    "\n",
    "<img src=\"images/Remaining Useful Life - RUL.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Pendekatan matematis VS ML: Apa Bedanya?</font></center>\n",
    "\n",
    "<center><img src=\"images/math_vs_ML_in_predictive_maintenance.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Reactive VS Preventive</font></center>\n",
    "\n",
    "<center><img src=\"images/Reactive VS Preventive.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Keputusan Strategis</font></center>\n",
    "\n",
    "* Biaya karena kerusakan/kegagalan biasanya lebih besar.\n",
    "* Maintenance rutin baik, namun terkadang terlalu cepat atau terlambat. Sehingga tidak optimal dari segi biaya.\n",
    "* Baseline AI-nya berarti dengan biaya maintenance saat ini.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_plus_ML.png\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Benefit - Keuntungan</font></center>\n",
    "\n",
    "* EFP-PM akan meminimumkan under/over maintenance, meningkatkan mutu layanan, meminimalkan resiko (hazard) akibat kerusakan alat, dsb.\n",
    "* EFP-PM yang baik dapat menghemat biaya hingga jutaan dolar pada industri yang besar.\n",
    "* Source: https://medium.com/swlh/machine-learning-for-equipment-failure-prediction-and-predictive-maintenance-pm-e72b1ce42da1\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_cost_benefit.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Pro-Contra Preventive Maintenance</font></center>\n",
    "\n",
    "<center><img src=\"images/Pro-Contra Preventive Maintenance.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Data, Data, Data</font></center>\n",
    "\n",
    "* Kita perlu data untuk memahami kerusakan. Data yang bermanfaat bisa berupa data statis/stasioner seperti cara kerja, rata-rata umur, atau \"operating condition\". Secara umum lebih banyak data tersedia lebih baik.\n",
    "* Data \"fine-grained\" yang dimonitor dalam interval waktu yang pendek lebih memungkinkan untuk digunakan sebagai prediksi.\n",
    "* Untuk menentukan data yang penting/relevan maka Data Scientist dan ahli (domain knowledge) harus bersama-sama mendiskusikannya.\n",
    "* Garbage-in-Garbage out: keep the data clean.\n",
    "* Data & Feature Engineering akan menjadi kunci utama.\n",
    "\n",
    "<center><img src=\"images/manufacturing_data.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Question Guidelines</font></center>\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_data_guideline_questions.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Pendekatan Utama Predictive Maintenance Dasar: Regresi & Klasifikasi</font></center>\n",
    "\n",
    "1. Regression models to predict remaining useful lifetime (RUL)\n",
    "2. Classification models to predict failure within a given time window\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_models.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Regression Approach to Predictive Maintenance</font></center>\n",
    "\n",
    "* \"Regression\": Tidak selalu OLS, tapi bisa juga Regression tree, SVR, Deep Learning, dsb.\n",
    "* Digunakan untuk memprediksi RUL: memprediksi variabel target yang bertipe numerik.\n",
    "* Data statis dan time series (historical) serta label dibutuhkan.\n",
    "* Satu model hanya digunakan untuk satu tipe failure. Jika ada beberapa kemungkinan failure harus dibuat beberapa model.\n",
    "\n",
    "<center><img src=\"images/Regression models to predict remaining useful lifetime.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Regression Approach to Predictive Maintenance 2</font></center>\n",
    "\n",
    "* **Source**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: operational_settings and sensor_measurements recorded for each cycle\n",
    "* **Model**: RandomForestRegressor \n",
    "* **Method**: Heavily rely on feature engineering via featuretools.\n",
    "\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_feature_tools.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Predict failure within a given time window</font></center>\n",
    "\n",
    "* Classification models can deal with multiple types of failure, as long as they are framed as a multi-class problem.\n",
    "* Since we are defining failure in a time window instead of an exact time, requirements related to the degradation process are different.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_classification.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Deep Learning Approach 01</font></center>\n",
    "\n",
    "* **Source**: https://towardsdatascience.com/system-failure-prediction-using-log-analysis-8eab84d56d1\n",
    "* **Data**: RAM, CPU and Hard Disk utilization\n",
    "* **Model**: RNN-LSTM\n",
    "* **Method**: Dimensional reduction (PCA) followed by a threshold (unsupervised approach yang telah dijelaskan Fathu di sesi sebelumnya).\n",
    "\n",
    "<center><img src=\"images/deep_learning_predictive_maintenance_PCA_LSTM.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Deep Learning Anomaly-Detection Based</font></center>\n",
    "\n",
    "* Time Series + Threshold. Triknya pada pemilihan atau rekayasa feature/variabel.\n",
    "* Figure shows an exponential degradation model that tracks failure in a high-speed bearing used in a wind turbine. The condition indicator is shown in blue. The degradation model predicts that the bearing will cross the threshold value in approximately 9.5 days. The region shaded in red represents the confidence bounds for this prediction.\n",
    "* Bisa untuk Latihan, menggunakan LSTM (sudah dibahas). \n",
    "* Link referensi: https://www.kaggle.com/r17sha/mtp-cmapps\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_via_anomaly_detection.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Deep Learning Approach 02</font></center>\n",
    "\n",
    "* **Source**: https://towardsdatascience.com/remaining-life-estimation-with-keras-2334514f9c61\n",
    "* **Data**: Turbofan Engine Degradation Simulation Dataset NASA\n",
    "* **Model**: Convolutional Neural network - CNN\n",
    "* **Method**: Merubah Data TIme-Series menjadi Image, lalu menggunakan kelebihan Deep Learning dalam menemukan pola pada image.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_cnn.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Contoh Penerapan 01: Feature Tools</font></center>\n",
    "\n",
    "* **Source**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: operational_settings and sensor_measurements recorded for each cycle\n",
    "* **Model**: RandomForestRegressor \n",
    "* **Method**: Heavily rely on feature engineering via featuretools.\n",
    "\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_feature_tools.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install featuretools composeml\n",
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import utils\n",
    "except:\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/utils.py\n",
    "    import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import numpy as np, pandas as pd, os\n",
    "import featuretools as ft, composeml as cp\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version =  2.6.0\n",
      "CUDA enabled TF, Num GPUs: 1 [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version = \", tf.__version__)\n",
    "if tf.test.is_built_with_cuda():\n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    print(\"CUDA enabled TF, Num GPUs:\", len(physical_devices), physical_devices) \n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "    except Exception as err_:\n",
    "        print(err_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = \"data/CMAPSS_Data_train_FD004.txt\"\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1,22)]\n",
    "col_names = index_names + setting_names + sensor_names\n",
    "try: # Running Locally, yakinkan \"file_\" berada di folder \"data\"\n",
    "    data = utils.load_data(file_)\n",
    "except: # Running in Google Colab\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/{file_}\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/data/CMAPSS_test_FD004.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/data/CMAPSS_RUL_FD004.txt\n",
    "    data = utils.load_data(file_)\n",
    "\n",
    "print(data.shape)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Data Understanding</font></center>\n",
    "\n",
    "\n",
    "* Turbofan Engine Degradation Simulation Dataset, provided by NASA, is becoming an important benchmark in the Remaining Useful Life (RUL) estimation for a fleet of engines of the same type (100 in total). Data are available in the form of time series: 3 operational settings, 21 sensor measurements and cycle — i.e. observations in terms of time for working life.\n",
    "The engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. The objective is to predict the number of remaining operational cycles before failure in the test set, i.e., the number of operational cycles after the last cycle that the engine will continue to operate.\n",
    "\n",
    "\n",
    "\n",
    "* In this dataset we have 249 engines (engine_no) which are monitored over time (time_in_cycles). \n",
    "* Each engine had operational_settings and sensor_measurements recorded for each cycle. \n",
    "* The Remaining Useful Life (RUL) is the amount of cycles an engine has left before it needs maintenance. \n",
    "* What makes this **dataset special** is that the engines run **all the way until failure**, giving us precise RUL information for every engine at every point in time.\n",
    "* To train a model that will **predict RUL**, we can can simulate real predictions on by choosing a random point in the life of the engine and **only using the data from before that point**. \n",
    "* We can create features with that restriction easily by using **cutoff_times** in **Featuretools**. \n",
    "* To structure the labeling process, we will use **Compose* which is an open source project for **automatically generating labels with cutoff times**.\n",
    "\n",
    "<center><img src=\"images/nasa Turbofan Engine Degradation Simulation.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Labeling Function: Hanya \"jumlah Baris - 1\"\n",
    "def remaining_useful_life(df):\n",
    "    return len(df) - 1\n",
    "\n",
    "# Contoh dengan Toy Data\n",
    "D = {'Name' : ['Ankit', 'Aishwarya', 'Shaurya', 'Shivangi'],\n",
    "    'Age' : [23, 21, 22, 21],\n",
    "    'University' : ['BHU', 'JNU', 'DU', 'BHU']} \n",
    "df = pd.DataFrame(D)\n",
    "print(df.shape)\n",
    "print(remaining_useful_life(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label maker\n",
    "\n",
    "* target_entity to the engine number. \n",
    "* By default, the window_size is set to the total observation size to contain the remaining observations for each engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = cp.LabelMaker(\n",
    "    target_entity='engine_no',\n",
    "    time_index='time',\n",
    "    labeling_function=remaining_useful_life,)\n",
    "type(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Labels \n",
    "\n",
    "## Automatic Data Labelling menggunakan Threshold dan \"Compose\"\n",
    "\n",
    "* Let’s imagine we want to make predictions on turbines that are up and running. \n",
    "* Turbines in general don’t fail before 120 cycles, so we will only make labels for engines that reach at least 100 cycles. \n",
    "* To do this, the minimum_data parameter is set to 100. \n",
    "* Using Compose, we can easily tweak this parameter as the requirements of our model changes. \n",
    "* By setting num_examples_per_instance to one, we limit the search to one example per engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_times = lm.search(\n",
    "    data.sort_values('time'),\n",
    "    num_examples_per_instance=1,\n",
    "    minimum_data=100,\n",
    "    verbose=True,)\n",
    "\n",
    "label_times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "* In the third row, we have engine number 3. At 00:00 on January 6, the remaining useful life of engine number 3 is 206. \n",
    "* Having a dataframe in this format tells Featuretools that the feature vector for engine number 3 should only be calculated with data from before that point in time.\n",
    "\n",
    "## Deep Feature Synthesis\n",
    "\n",
    "* To apply Deep Feature Synthesis we need to establish an EntitySet structure for our data. The key insight in this step is that we're really interested in our data as collected by engine. We can create an engines entity by normalizing by the engine_no column in the raw data. In the next section, we'll create a feature matrix for the engines entity directly rather than the base dataframe of recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entityset(data):\n",
    "    es = ft.EntitySet('Dataset')\n",
    "    es.entity_from_dataframe(\n",
    "        dataframe=data,\n",
    "        entity_id='recordings',\n",
    "        index='index',\n",
    "        time_index='time',)\n",
    "    es.normalize_entity(\n",
    "        base_entity_id='recordings',\n",
    "        new_entity_id='engines',\n",
    "        index='engine_no',)\n",
    "    es.normalize_entity(\n",
    "        base_entity_id='recordings',\n",
    "        new_entity_id='cycles',\n",
    "        index='time_in_cycles',)\n",
    "    return es\n",
    "\n",
    "es = make_entityset(data)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS and Creating a Model\n",
    "\n",
    "* With the work from the last section in hand, we can quickly build features using Deep Feature Synthesis (DFS). The function ft.dfs takes an EntitySet and stacks primitives like Max, Min and Last exhaustively across entities. Feel free to try the next step with a different primitive set to see how the results differ!\n",
    "\n",
    "* We build features only using data up to and including the cutoff time of each label. This is done by setting the cutoff_time parameter to the label times we generated previously. Notice that the output of Compose integrates easily with Featuretools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, features = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='engines',\n",
    "    agg_primitives=['last', 'max', 'min'],\n",
    "    trans_primitives=[],\n",
    "    cutoff_time=label_times,\n",
    "    max_depth=3,\n",
    "    verbose=True,)\n",
    "\n",
    "fm.to_csv('simple_fm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Baselines\n",
    "\n",
    "Before we use that feature matrix to make predictions, we should check how well guessing does on this dataset. We can use a train_test_split from scikit-learn to split our training data once and for all. Then, we'll check the following baselines:\n",
    "\n",
    "1. Always predict the median value of y_train\n",
    "2. Always predict the RUL as if every engine has the median lifespan in X_train\n",
    "\n",
    "We'll check those predictions by finding the mean of the absolute value of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('simple_fm.csv', index_col='engine_no')\n",
    "X = fm.copy().fillna(0)\n",
    "y = X.pop('remaining_useful_life')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "\n",
    "medianpredict1 = [np.median(y_train) for _ in y_test]\n",
    "mae = mean_absolute_error(medianpredict1, y_test)\n",
    "print('Baseline by median label: Mean Abs Error = {:.2f}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_train = es['recordings'].df['engine_no'].isin(y_train.index)\n",
    "recordings_from_train = es['recordings'].df[from_train]\n",
    "engines = recordings_from_train.groupby(['engine_no'])\n",
    "median_life = np.median(engines.apply(lambda df: df.shape[0]))\n",
    "\n",
    "from_test = es['recordings'].df['engine_no'].isin(y_test.index)\n",
    "recordings_from_test = es['recordings'].df[from_test]\n",
    "engines = recordings_from_test.groupby(['engine_no'])\n",
    "life_in_test = engines.apply(lambda df: df.shape[0]) - y_test\n",
    "\n",
    "medianpredict2 = median_life - life_in_test\n",
    "medianpredict2 = medianpredict2.apply(lambda row: max(row, 0))\n",
    "mae = mean_absolute_error(medianpredict2, y_test)\n",
    "print('Baseline by median life: Mean Abs Error = {:.2f}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Using the Model\n",
    "\n",
    "Now, we can use our created features to fit a RandomForestRegressor to our data and see if we can improve on the previous scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators=100)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "preds = reg.predict(X_test)\n",
    "scores = mean_absolute_error(preds, y_test)\n",
    "print('Mean Abs Error: {:.2f}'.format(scores))\n",
    "\n",
    "high_imp_feats = utils.feature_importances(X, reg, feats=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we can apply the exact same transformations (including DFS) to our test data. For this particular case, the real answer isn't in the data so we don't need to worry about cutoff times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = utils.load_data('data/CMAPSS_test_FD004.txt')\n",
    "es2 = make_entityset(data2)\n",
    "\n",
    "fm2 = ft.calculate_feature_matrix(\n",
    "    entityset=es2,\n",
    "    features=features,\n",
    "    verbose=True,)\n",
    "\n",
    "fm2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fm2.copy().fillna(0)\n",
    "y = pd.read_csv('data/CMAPSS_RUL_FD004.txt', sep=' ', header=None, names=['remaining_useful_life'],index_col=False,)\n",
    "\n",
    "preds2 = reg.predict(X)\n",
    "mae = mean_absolute_error(preds2, y)\n",
    "print('Mean Abs Error: {:.2f}'.format(mae))\n",
    "\n",
    "medianpredict1 = [np.median(y_train) for _ in preds2]\n",
    "mae = mean_absolute_error(medianpredict1, y)\n",
    "print('Baseline by median label: Mean Abs Error = {:.2f}'.format(mae))\n",
    "\n",
    "engines = es2['recordings'].df.groupby(['engine_no'])\n",
    "medianpredict2 = median_life - engines.apply(lambda df: df.shape[0])\n",
    "medianpredict2 = medianpredict2.apply(lambda row: max(row, 0))\n",
    "mae = mean_absolute_error(medianpredict2, y)\n",
    "print('Baseline by median life: Mean Abs Error = {:.2f}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sama seperti $R^2$ di Regresi, Evaluasi ML termudah adalah dengan membandingkan dengan Baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Contoh Penerapan 02: CNN</font></center>\n",
    "\n",
    "\n",
    "* Source: https://towardsdatascience.com/remaining-life-estimation-with-keras-2334514f9c61\n",
    "* Data: Turbofan Engine Degradation Simulation Dataset NASA\n",
    "* Model: Convolutional Neural network - CNN\n",
    "* Method: Merubah Data TIme-Series menjadi Image, lalu menggunakan kelebihan Deep Learning dalam menemukan pola pada image.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_cnn.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1Kj5s0dQGeTu",
    "outputId": "2bcb7083-9128-46fe-a2a7-ef3ce0dfbbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#id: 100\n",
      "(20631, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99  1404.20   \n",
       "\n",
       "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
       "0  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03  392  2388  100.0   \n",
       "1  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03  392  2388  100.0   \n",
       "2  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03  390  2388  100.0   \n",
       "\n",
       "     s20      s21  \n",
       "0  39.06  23.4190  \n",
       "1  39.00  23.4236  \n",
       "2  38.95  23.3442  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LOAD TRAIN ###\n",
    "file_ = 'data/RLE_PM_train.txt'\n",
    "try:\n",
    "    train_df = pd.read_csv(file_, sep=\" \", header=None)\n",
    "except: # Running in Google Colab\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/{file_}\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/data/RLE_PM_test.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/ptpjb/master/data/RLE_PM_truth.txt\n",
    "    data = utils.load_data(file_)\n",
    "    \n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "print('#id:',len(train_df.id.unique()))\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "print(train_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ALSdMiIkGeTv",
    "outputId": "7c818c83-7f52-4aa6-e5ea-51059a54ef15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium working time: 206.31\n",
      "max working time: 362\n",
      "min working time: 128\n"
     ]
    }
   ],
   "source": [
    "### PLOT TRAIN FREQ ###\n",
    "plt.figure(figsize=(20,6))\n",
    "train_df.id.value_counts().plot.bar()\n",
    "print(\"medium working time:\", train_df.id.value_counts().mean())\n",
    "print(\"max working time:\", train_df.id.value_counts().max())\n",
    "print(\"min working time:\", train_df.id.value_counts().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8au1KYhxGeTw",
    "outputId": "52882b5e-c8f2-4ec0-a109-1a8681ff4803"
   },
   "outputs": [],
   "source": [
    "### plotting sensor data for engine ID ###\n",
    "engine_id = train_df[train_df['id'] == 1]\n",
    "\n",
    "ax1 = engine_id[train_df.columns[2:]].plot(subplots=True, sharex=True, figsize=(20,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rGqxRgSfGeTw",
    "outputId": "457ef33a-1c68-4581-8995-0355f2c703e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#id: 100\n",
      "(13096, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.02</td>\n",
       "      <td>1585.29</td>\n",
       "      <td>1398.21</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.72</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8125.55</td>\n",
       "      <td>8.4052</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.86</td>\n",
       "      <td>23.3735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.71</td>\n",
       "      <td>1588.45</td>\n",
       "      <td>1395.42</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.16</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8139.62</td>\n",
       "      <td>8.3803</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.02</td>\n",
       "      <td>23.3916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.46</td>\n",
       "      <td>1586.94</td>\n",
       "      <td>1401.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.97</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8130.10</td>\n",
       "      <td>8.4441</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.08</td>\n",
       "      <td>23.4166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1    0.0023    0.0003     100.0  518.67  643.02  1585.29  1398.21   \n",
       "1   1      2   -0.0027   -0.0003     100.0  518.67  641.71  1588.45  1395.42   \n",
       "2   1      3    0.0003    0.0001     100.0  518.67  642.46  1586.94  1401.34   \n",
       "\n",
       "      s5  ...     s12      s13      s14     s15   s16  s17   s18    s19  \\\n",
       "0  14.62  ...  521.72  2388.03  8125.55  8.4052  0.03  392  2388  100.0   \n",
       "1  14.62  ...  522.16  2388.06  8139.62  8.3803  0.03  393  2388  100.0   \n",
       "2  14.62  ...  521.97  2388.03  8130.10  8.4441  0.03  393  2388  100.0   \n",
       "\n",
       "     s20      s21  \n",
       "0  38.86  23.3735  \n",
       "1  39.02  23.3916  \n",
       "2  39.08  23.4166  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LOAD TEST ###\n",
    "test_df = pd.read_csv('data/RLE_PM_test.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "print('#id:',len(test_df.id.unique()))\n",
    "print(test_df.shape)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eWJhO9JqGeTx",
    "outputId": "6717e1dd-60b0-46af-f994-df6cc0c7b48f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>more</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   more\n",
       "1   112\n",
       "2    98\n",
       "3    69"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### LOAD GROUND TRUTH ###\n",
    "truth_df = pd.read_csv('data/RLE_PM_truth.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "truth_df.columns = ['more']\n",
    "truth_df = truth_df.set_index(truth_df.index + 1)\n",
    "\n",
    "print(truth_df.shape)\n",
    "truth_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pfDdV41tGeTx",
    "outputId": "2c7adced-4e6a-480f-ce5f-8f1750cce00b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    191\n",
       "1    190\n",
       "2    189\n",
       "3    188\n",
       "4    187\n",
       "5    186\n",
       "6    185\n",
       "7    184\n",
       "8    183\n",
       "9    182\n",
       "Name: RUL, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CALCULATE RUL TRAIN ###\n",
    "train_df['RUL']=train_df.groupby(['id'])['cycle'].transform(max)-train_df['cycle']\n",
    "train_df.RUL[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qEO9MBahGeTx"
   },
   "outputs": [],
   "source": [
    "### ADD NEW LABEL TRAIN ###\n",
    "w1 = 45\n",
    "w0 = 15\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cH7i148qGeTy",
    "outputId": "45e01f22-4dd2-4bbf-b726-678cb03823ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s17</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.406802</td>\n",
       "      <td>0.309757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633262</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.199608</td>\n",
       "      <td>0.363986</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.724662</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.453019</td>\n",
       "      <td>0.352633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765458</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731014</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.369523</td>\n",
       "      <td>0.370527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.171793</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.621375</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.256159</td>\n",
       "      <td>0.331195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>0.166603</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.662386</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.404625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.668277</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.402078</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.704502</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2        s2        s3        s4   s6        s7  \\\n",
       "0   1      1  0.459770  0.166667  0.183735  0.406802  0.309757  1.0  0.726248   \n",
       "1   1      2  0.609195  0.250000  0.283133  0.453019  0.352633  1.0  0.628019   \n",
       "2   1      3  0.252874  0.750000  0.343373  0.369523  0.370527  1.0  0.710145   \n",
       "3   1      4  0.540230  0.500000  0.343373  0.256159  0.331195  1.0  0.740741   \n",
       "4   1      5  0.390805  0.333333  0.349398  0.257467  0.404625  1.0  0.668277   \n",
       "\n",
       "         s8  ...       s12       s13       s14       s15       s17       s20  \\\n",
       "0  0.242424  ...  0.633262  0.205882  0.199608  0.363986  0.333333  0.713178   \n",
       "1  0.212121  ...  0.765458  0.279412  0.162813  0.411312  0.333333  0.666667   \n",
       "2  0.272727  ...  0.795309  0.220588  0.171793  0.357445  0.166667  0.627907   \n",
       "3  0.318182  ...  0.889126  0.294118  0.174889  0.166603  0.333333  0.573643   \n",
       "4  0.242424  ...  0.746269  0.235294  0.174734  0.402078  0.416667  0.589147   \n",
       "\n",
       "        s21  RUL  label1  label2  \n",
       "0  0.724662  191       0       0  \n",
       "1  0.731014  190       0       0  \n",
       "2  0.621375  189       0       0  \n",
       "3  0.662386  188       0       0  \n",
       "4  0.704502  187       0       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SCALE TRAIN DATA ###\n",
    "\n",
    "def scale(df):\n",
    "    #return (df - df.mean())/df.std()\n",
    "    return (df - df.min())/(df.max()-df.min())\n",
    "\n",
    "for col in train_df.columns:\n",
    "    if col[0] == 's':\n",
    "        train_df[col] = scale(train_df[col])\n",
    "#     elif col == 'cycle':\n",
    "#         train_df['cycle_norm'] = scale(train_df[col])\n",
    "        \n",
    "train_df = train_df.dropna(axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TlNn080PGeTy"
   },
   "outputs": [],
   "source": [
    "### CALCULATE RUL TEST ###\n",
    "truth_df['max'] = test_df.groupby('id')['cycle'].max() + truth_df['more']\n",
    "test_df['RUL'] = [truth_df['max'][i] for i in test_df.id] - test_df['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lXZ55c6wGeTz"
   },
   "outputs": [],
   "source": [
    "### ADD NEW LABEL TEST ###\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WW_bkenCGeTz",
    "outputId": "cca724bb-6c76-43ce-c92f-18d28da2f198"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>...</th>\n",
       "      <th>s12</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s17</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65625</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.596215</td>\n",
       "      <td>0.421968</td>\n",
       "      <td>0.282214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608871</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.152259</td>\n",
       "      <td>0.347076</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.620099</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.182965</td>\n",
       "      <td>0.504025</td>\n",
       "      <td>0.225240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800403</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634703</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.277907</td>\n",
       "      <td>0.227709</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.645718</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.53125</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.419558</td>\n",
       "      <td>0.464814</td>\n",
       "      <td>0.346130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.651210</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591324</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.192892</td>\n",
       "      <td>0.533557</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.681104</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.77500</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.413249</td>\n",
       "      <td>0.391587</td>\n",
       "      <td>0.449867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.643145</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456621</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.217896</td>\n",
       "      <td>0.282359</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.627273</td>\n",
       "      <td>0.620382</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.435331</td>\n",
       "      <td>0.471306</td>\n",
       "      <td>0.357974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632420</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.187891</td>\n",
       "      <td>0.337009</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.676008</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2        s2        s3        s4   s6        s7  \\\n",
       "0   1      1   0.65625  0.692308  0.596215  0.421968  0.282214  1.0  0.608871   \n",
       "1   1      2   0.34375  0.230769  0.182965  0.504025  0.225240  1.0  0.800403   \n",
       "2   1      3   0.53125  0.538462  0.419558  0.464814  0.346130  1.0  0.651210   \n",
       "3   1      4   0.77500  0.461538  0.413249  0.391587  0.449867  1.0  0.643145   \n",
       "4   1      5   0.60000  0.461538  0.435331  0.471306  0.357974  1.0  0.661290   \n",
       "\n",
       "         s8  ...       s12       s13       s14       s15    s17       s20  \\\n",
       "0  0.365854  ...  0.534247  0.325581  0.152259  0.347076  0.375  0.500000   \n",
       "1  0.292683  ...  0.634703  0.395349  0.277907  0.227709  0.500  0.645455   \n",
       "2  0.390244  ...  0.591324  0.325581  0.192892  0.533557  0.500  0.700000   \n",
       "3  0.341463  ...  0.456621  0.372093  0.217896  0.282359  0.250  0.627273   \n",
       "4  0.292683  ...  0.632420  0.325581  0.187891  0.337009  0.125  0.618182   \n",
       "\n",
       "        s21  RUL  label1  label2  \n",
       "0  0.620099  142       0       0  \n",
       "1  0.645718  141       0       0  \n",
       "2  0.681104  140       0       0  \n",
       "3  0.620382  139       0       0  \n",
       "4  0.676008  138       0       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SCALE TEST DATA ###\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if col[0] == 's':\n",
    "        test_df[col] = scale(test_df[col])\n",
    "#     elif col == 'cycle':\n",
    "#         test_df['cycle_norm'] = scale(test_df[col])\n",
    "        \n",
    "test_df = test_df.dropna(axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xSDOqXCGeTz"
   },
   "source": [
    "# GEN SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W0nHdnl5GeT0"
   },
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,142),(50,192)\n",
    "    # 0 50 (start stop) -> from row 0 to row 50\n",
    "    # 1 51 (start stop) -> from row 1 to row 51\n",
    "    # 2 52 (start stop) -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 141 191 (start stop) -> from row 141 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "def gen_labels(id_df, seq_length, label):\n",
    "\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EsE379XNGeT1",
    "outputId": "018a67f4-b1e5-4c57-caf1-d34f8b3d5fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setting1', 'setting2', 's2', 's3', 's4', 's6', 's7', 's8', 's9', 's11', 's12', 's13', 's14', 's15', 's17', 's20', 's21']\n"
     ]
    }
   ],
   "source": [
    "### SEQUENCE COL: COLUMNS TO CONSIDER ###\n",
    "sequence_cols = []\n",
    "for col in train_df.columns:\n",
    "    if col[0] == 's':\n",
    "        sequence_cols.append(col)\n",
    "#sequence_cols.append('cycle_norm')\n",
    "print(sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xlykTAW2GeT2",
    "outputId": "1df79983-b282-4ab8-d762-0a29fd01a107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train shape: (15631, 50, 17)\n",
      "X_Test shape: (8162, 50, 17)\n"
     ]
    }
   ],
   "source": [
    "### GENERATE X TRAIN TEST ###\n",
    "x_train, x_test = [], []\n",
    "for engine_id in train_df.id.unique():\n",
    "    for sequence in gen_sequence(train_df[train_df.id==engine_id], sequence_length, sequence_cols):\n",
    "        x_train.append(sequence)\n",
    "    for sequence in gen_sequence(test_df[test_df.id==engine_id], sequence_length, sequence_cols):\n",
    "        x_test.append(sequence)\n",
    "    \n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "print(\"X_Train shape:\", x_train.shape)\n",
    "print(\"X_Test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rmSrqg5OGeT2",
    "outputId": "17858e85-c89f-44fb-f710-74b978c997df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (15631, 1)\n",
      "y_test shape: (8162, 1)\n"
     ]
    }
   ],
   "source": [
    "### GENERATE Y TRAIN TEST ###\n",
    "y_train, y_test = [], []\n",
    "for engine_id in train_df.id.unique():\n",
    "    for label in gen_labels(train_df[train_df.id==engine_id], sequence_length, ['label2'] ):\n",
    "        y_train.append(label)\n",
    "    for label in gen_labels(test_df[test_df.id==engine_id], sequence_length, ['label2']):\n",
    "        y_test.append(label)\n",
    "    \n",
    "y_train = np.asarray(y_train).reshape(-1,1)\n",
    "y_test = np.asarray(y_test).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1Wbb4KnPGeT3",
    "outputId": "895b1416-0713-4c3f-8f90-7af5e710abb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15631, 3)\n",
      "(8162, 3)\n"
     ]
    }
   ],
   "source": [
    "### ENCODE LABEL ###\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Ag3wnxGeT3"
   },
   "source": [
    "# FROM TIME SERIES TO IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oRb4rEfsGeT3"
   },
   "outputs": [],
   "source": [
    "def rec_plot(s, eps=0.10, steps=10):\n",
    "    d = pdist(s[:,None])\n",
    "    d = np.floor(d/eps)\n",
    "    d[d>steps] = steps\n",
    "    Z = squareform(d)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qhaLDhseGeT4",
    "outputId": "442ef74e-137b-470d-d3e5-440e10a966e6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(0,17):\n",
    "    plt.subplot(6, 3, i+1)    \n",
    "    rec = rec_plot(x_train[0,:,i])\n",
    "    plt.imshow(rec)\n",
    "    plt.title(sequence_cols[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qy0-jXHDGeT4",
    "outputId": "f0c8ffa2-82e2-45ea-f200-ef8b8257a3c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15631, 50, 50, 17)\n",
      "(8162, 50, 50, 17)\n"
     ]
    }
   ],
   "source": [
    "### TRANSFORM X TRAIN TEST IN IMAGES ###\n",
    "x_train_img = np.apply_along_axis(rec_plot, 1, x_train).astype('float16')\n",
    "print(x_train_img.shape)\n",
    "\n",
    "x_test_img = np.apply_along_axis(rec_plot, 1, x_test).astype('float16')\n",
    "print(x_test_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt2sQwxkGeT5"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rFhRjqICGeT5"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "peFQvCSyGeT5",
    "outputId": "4d3244ef-c545-4fd3-fc40-09d885926228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        4928      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 19, 19, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1327360   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 1,397,731\n",
      "Trainable params: 1,397,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 17)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HfZgkBSYGeT5",
    "outputId": "775ad6e9-003f-4b0c-be61-450ef51ce95f"
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14416/2490236854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m                **kwargs):\n\u001b[0;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1429\u001b[0m   \"\"\"\n\u001b[0;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[1;32m-> 1431\u001b[1;33m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1439\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 272\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m   \u001b[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\Py37\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "set_seed(33)\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='auto', restore_best_weights=True, verbose=1, patience=6)\n",
    "\n",
    "model.fit(x_train_img, y_train, batch_size=4, epochs=25, callbacks=[es],validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4LkZmlxGeT6",
    "outputId": "2a1f017e-516a-4fbe-fcaa-2f07e7966d0d"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test_img, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuClHKYTGeT6"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 14)\n",
    "\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAU_xT3tGeT7",
    "outputId": "0569628f-a766-44e3-edac-addb58d8b839"
   },
   "outputs": [],
   "source": [
    "preds_prob = model.predict(x_test_img)\n",
    "preds = np.argmax(preds_prob)\n",
    "print(classification_report(np.where(y_test != 0)[1], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UytBuqiHGeT7",
    "outputId": "dbc21ff8-e4b6-4448-d094-86e54795041f"
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(np.where(y_test != 0)[1], model.predict_classes(x_test_img))\n",
    "plt.figure(figsize=(7,7))\n",
    "plot_confusion_matrix(cnf_matrix, classes=np.unique(np.where(y_test != 0)[1]), title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Akhir Modul 16 - Machine Learning untuk Prediksi Kerusakan Komponen</font></center>\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/meme-no-predictive-maintenance-what-year-is-it.jpg\" style=\"height: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
